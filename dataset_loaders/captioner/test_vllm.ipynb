{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.assets.image import ImageAsset \n",
    "from vllm.assets.video import VideoAsset\n",
    "from vllm.utils import FlexibleArgumentParser\n",
    "# Specify the maximum number of frames per video to be 4. This can be changed.\n",
    "# llm = LLM(\"Qwen/Qwen2.5-VL-3B-Instruct\", limit_mm_per_prompt={\"image\": 4}, max_model_len=10000)\n",
    "llm = LLM(\n",
    "        model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "        max_model_len=8000,\n",
    "        max_num_seqs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.7,\n",
    "                                     max_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"HuggingFaceFV/finevideo\", split=\"train\")\n",
    "\n",
    "# Access the first video\n",
    "video = ds[0][\"mp4\"]\n",
    "\n",
    "#full dataset (600GB of data)\n",
    "#dataset = load_dataset(\"HuggingFaceFV/finevideo/\", split=\"train\", num_proc=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Specify a dataset of video (100 videos)\n",
    "# 2. Specify a prompt that asks the model to generate a description of the video * 100\n",
    "# 3. You should be able to feed all of it together at once to the model to generate (DO NOT FEED ONE BY ONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the videos \n",
    "dataset_path = \"/home/ubuntu/temp/10k_vid_dataset/mp4s\"\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "num_videos = 50\n",
    "videos = []\n",
    "num_chunks = 3\n",
    "time_per_chunk = 10\n",
    "for i in tqdm(range(num_videos), total=num_videos):\n",
    "    #save video \n",
    "    video_path = os.path.join(dataset_path, f\"video_{i}.mp4\")\n",
    "    with open(video_path, \"wb\") as f:\n",
    "        f.write(ds[i][\"mp4\"])\n",
    "    # read video    \n",
    "    vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    vid_dict = {\"vid_path\": video_path}\n",
    "    success = True\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        try:\n",
    "            frame_idxs = np.arange(chunk_idx * time_per_chunk * 30, (chunk_idx + 1) * time_per_chunk * 30, 30)\n",
    "            nd_frames = vr.get_batch(frame_idxs).asnumpy()\n",
    "            vid_dict[f\"vT{chunk_idx}\"] = nd_frames\n",
    "        except:\n",
    "            print(f\"Error processing\")\n",
    "            success = False\n",
    "    if(success):\n",
    "        videos.append(vid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA STRUCTURE\n",
    "# videos = [{\"vid_path\": \"path...\",\n",
    "#            \"vid\": [frame1, frame2, frame3, ...],\n",
    "#            \"T1\": {\"description\": {\"Q\": \"text...\", \"A\": \"text...\"},\n",
    "#                   \"temporal\": {\"time\": int, \"content\": \"text...\"}},\n",
    "#                   \"question\": {\"Q\": \"text...\", \"A\": \"text...\"},\n",
    "#           }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.assets.video import VideoAsset\n",
    "\n",
    "def generate_video_description(video_frame_list, prompt, num_chunks):\n",
    "    messages = []\n",
    "    for idx, dict_obj in enumerate(video_frame_list):\n",
    "        for chunk_idx in range(num_chunks):\n",
    "            if(type(prompt) == list):\n",
    "                prompt_curr = prompt[idx*num_chunks + chunk_idx]\n",
    "            else:\n",
    "                prompt_curr = prompt\n",
    "            message = {\"prompt\": prompt_curr, \"multi_modal_data\": {\"video\": dict_obj[f\"vT{chunk_idx}\"]}}\n",
    "            messages.append(message)\n",
    "    # Perform inference and log output.\n",
    "    print(\"Starting inference on \", len(messages), \" messages\")\n",
    "    outputs = llm.generate(messages, sampling_params=sampling_params)\n",
    "    generated_texts = []\n",
    "    for idx in range(len(outputs)):\n",
    "        generated_text = outputs[idx].outputs[0].text\n",
    "        generated_texts.append(generated_text)\n",
    "    print(\"Generated\", len(generated_texts), \"texts\")\n",
    "    return generated_texts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_describe = (\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "              f\"<|im_start|>user\\n<|vision_start|><|video_pad|><|vision_end|>\"\n",
    "              f\"Describe what is shown in the video. <|im_end|>\\n\"\n",
    "              \"<|im_start|>assistant\\n\")\n",
    "\n",
    "generated_texts = generate_video_description(videos, prompt_describe, num_chunks)\n",
    "\n",
    "for idx, text in enumerate(generated_texts):\n",
    "    chunk_idx = idx%num_chunks\n",
    "    list_idx = idx//num_chunks\n",
    "    start_time = chunk_idx * time_per_chunk\n",
    "    end_time = (chunk_idx + 1) * time_per_chunk\n",
    "    print(f\"Video {list_idx} chunk {chunk_idx} time {start_time} to {end_time} seconds\")\n",
    "    videos[list_idx][f\"T{chunk_idx}_description\"] = {\"Q\": f\"What occurs between {start_time} and {end_time} seconds?\", \"A\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(videos[-1][\"vid_path\"])\n",
    "for key in videos[-1].keys():\n",
    "    if(\"description\" in key):\n",
    "        print(f\"{key}: {videos[-1][key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Step Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_timestep = (\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "              f\"<|im_start|>user\\n<|vision_start|><|video_pad|><|vision_end|>\"\n",
    "              \"What occurs at time {time} seconds?<|im_end|>\\n\"\n",
    "              \"<|im_start|>assistant\\n\")\n",
    "\n",
    "sample_time_steps = np.random.randint(0, time_per_chunk, num_videos*num_chunks)\n",
    "\n",
    "prompt_list_timesteps = []\n",
    "for time_step in sample_time_steps:\n",
    "    prompt = prompt_timestep.replace(\"{time}\", str(time_step))\n",
    "    prompt_list_timesteps.append(prompt)\n",
    "\n",
    "generated_texts = generate_video_description(videos, prompt_list_timesteps, num_chunks)\n",
    "\n",
    "for idx, text in enumerate(generated_texts):\n",
    "    chunk_idx = idx%num_chunks\n",
    "    list_idx = idx//num_chunks\n",
    "    vid_time = sample_time_steps[idx]\n",
    "    calc_time = chunk_idx*time_per_chunk+sample_time_steps[idx] # have to recalculate due to chunking\n",
    "    videos[list_idx][f\"T{chunk_idx}_timestep\"] = {\"Q:\": f\"What occurs at time {calc_time} seconds?\", \"A\": text.replace(str(vid_time), str(calc_time))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(videos[0].keys())\n",
    "for key in videos[0].keys():\n",
    "    if(\"timestep\" in key):\n",
    "        print(f\"{key}: {videos[0][key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_gen_question =  (\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "              f\"<|im_start|>user\\n<|vision_start|><|video_pad|><|vision_end|>\"\n",
    "              f\"What is a vision understanding question you can ask about the visual content of the video. Only output the question<|im_end|>\\n\"\n",
    "              \"<|im_start|>assistant\\n\")\n",
    "\n",
    "generated_texts = generate_video_description(videos, prompt_gen_question, num_chunks)\n",
    "\n",
    "questions = generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_answer_question =  (\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "              f\"<|im_start|>user\\n<|vision_start|><|video_pad|><|vision_end|>\"\n",
    "              \"{question}<|im_end|>\\n\"\n",
    "              \"<|im_start|>assistant\\n\")\n",
    "\n",
    "\n",
    "prompt_qas = []\n",
    "for q in questions:\n",
    "    qa = prompt_answer_question.replace(\"{question}\", q)\n",
    "    prompt_qas.append(qa)\n",
    "\n",
    "\n",
    "generated_texts = generate_video_description(videos, prompt_qas, num_chunks)\n",
    "answers = generated_texts\n",
    "\n",
    "for idx in range(len(answers)):\n",
    "    chunk_idx = idx%num_chunks\n",
    "    list_idx = idx//num_chunks\n",
    "    q = questions[idx]\n",
    "    a = answers[idx]\n",
    "    videos[list_idx][f\"T{chunk_idx}_qa\"] = {\"Q\": q, \"A\": a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_set = videos[-3]\n",
    "print(vid_set.keys())\n",
    "for key in vid_set.keys():\n",
    "    if(\"qa\" in key):\n",
    "        print(\"Q\", vid_set[key][\"Q\"])\n",
    "        print(\"A\", vid_set[key][\"A\"])\n",
    "        print(\"______________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_set = videos[-8]\n",
    "print(vid_set.keys())\n",
    "print(vid_set[\"vid_path\"])\n",
    "for key in vid_set.keys():\n",
    "    if(\"vT\" not in key):\n",
    "        print(key, vid_set[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to huggingface dictionary\n",
    "from datasets import Dataset\n",
    "root_path = \"/home/ubuntu/temp/10k_vid_dataset/\"\n",
    "#recreate data list without video frames\n",
    "data_list = []\n",
    "for vid in videos:\n",
    "    vid_dict = {}\n",
    "    for key in vid.keys():\n",
    "        if(\"vT\" not in key):\n",
    "            vid_dict[key] = vid[key]\n",
    "    data_list.append(vid_dict)\n",
    "#save to huggingface dataset\n",
    "dataset = Dataset.from_list(data_list)\n",
    "dataset.save_to_disk(os.path.join(root_path, \"finevideo_dataset\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
