{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, AutoModel, AutoImageProcessor\n",
    "\n",
    "model_name = \"DAMO-NLP-SG/VideoLLaMA3-2B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "video_path = \"put your video path here\"\n",
    "image_path = \"put your image path here\"\n",
    "question = \"Describe this video in detail.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The man in the video is not named.\n"
     ]
    }
   ],
   "source": [
    "# Video conversation\n",
    "video_path = \"/home/ubuntu/sample.mp4\"\n",
    "#image_path = \"/home/ubuntu/45s.png\"\n",
    "image_path = \"/home/ubuntu/graduation.png\"\n",
    "\n",
    "\n",
    "#sample question\n",
    "#question = \"What happens at time: 10 seconds?\"\n",
    "\n",
    "\n",
    "#image based QA\n",
    "#image_caption = \"A modern educational or institutional building with a green lawn, featuring a white facade and large glass windows under a clear blue sky. A police emblem watermark is visible in the upper right corner\"\n",
    "question = \"What's his name\"\n",
    "\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful and informed assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"video\", \"video\": {\"video_path\": video_path, \"fps\": 1, \"max_frames\": 1, \"start_time\":50.0, \"end_time\":70.0}},\n",
    "            {\"type\": \"text\", \"text\": question},\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "inputs = processor(conversation=conversation, return_tensors=\"pt\")\n",
    "inputs = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "if \"pixel_values\" in inputs:\n",
    "    inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(torch.bfloat16)\n",
    "output_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "response = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Past KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.forward(**inputs, return_dict=True, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,    323,  15987,\n",
       "           17847,     13, 151645,    198, 151644,    872,    198,   1462,    220,\n",
       "              20,     15,     13,     15,     82,     25, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
       "          151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,    198,\n",
       "            3838,    594,    806,    829, 151645,    198]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "        device='cuda:0'),\n",
       " 'pixel_values': tensor([[-0.1455, -0.1455, -0.1377,  ..., -0.4668, -0.4746, -0.4824],\n",
       "         [-0.0747, -0.0747, -0.0747,  ..., -0.2793, -0.2637, -0.2793],\n",
       "         [-0.3730, -0.3809, -0.4043,  ..., -0.3027, -0.3105, -0.3105],\n",
       "         ...,\n",
       "         [ 0.8828,  0.8828,  0.8750,  ...,  0.4512,  0.4590,  0.4590],\n",
       "         [ 0.8750,  0.8672,  0.8672,  ...,  0.4668,  0.4590,  0.4590],\n",
       "         [ 0.8672,  0.8672,  0.8672,  ...,  0.4590,  0.4590,  0.4590]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'grid_sizes': tensor([[ 1, 26, 46]], device='cuda:0'),\n",
       " 'merge_sizes': tensor([2], device='cuda:0'),\n",
       " 'modals': ['video']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 330, 128])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.past_key_values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1196, 588])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding in KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding special Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [MEMORY], Token ID: 151668\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# Initialize the tokenizer\n",
    "model_id = \"DAMO-NLP-SG/VideoLLaMA3-2B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "new_tokens = [\"[MEMORY]\"]\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": new_tokens})\n",
    "\n",
    "# Check if the tokens are in the vocabulary\n",
    "for token in new_tokens:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    print(f\"Token: {token}, Token ID: {token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['q', 'wer', 'Ġq', 'wer', 'Ġq', 'wer', 'Ġ', '[MEMORY]', '[MEMORY]']\n",
      "[80, 6566, 2804, 6566, 2804, 6566, 220, 151668, 151668]\n",
      "['q', 'wer', 'Ġq', 'wer', 'Ġq', 'wer', 'Ġ', '[MEMORY]', '[MEMORY]']\n"
     ]
    }
   ],
   "source": [
    "# Check that it works\n",
    "test_str = \"qwer qwer qwer [MEMORY][MEMORY]\"\n",
    "tokens = tokenizer.tokenize(test_str)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "reconstruct_tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "print(tokens)\n",
    "print(ids)\n",
    "print(reconstruct_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>',\n",
       " 'system',\n",
       " 'Ċ',\n",
       " 'You',\n",
       " 'Ġare',\n",
       " 'Ġa',\n",
       " 'Ġhelpful',\n",
       " 'Ġand',\n",
       " 'Ġinformed',\n",
       " 'Ġassistant',\n",
       " '.',\n",
       " '<|im_end|>',\n",
       " 'Ċ',\n",
       " '<|im_start|>',\n",
       " 'user',\n",
       " 'Ċ',\n",
       " 'Time',\n",
       " 'Ġ',\n",
       " '5',\n",
       " '0',\n",
       " '.',\n",
       " '0',\n",
       " 's',\n",
       " ':',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " '<image>',\n",
       " 'Ċ',\n",
       " 'What',\n",
       " \"'s\",\n",
       " 'Ġhis',\n",
       " 'Ġname',\n",
       " '<|im_end|>',\n",
       " 'Ċ']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# collate_fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6247ded383dd44308183f63fda7cad78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "hf_dataset = load_from_disk(\"/home/ubuntu/temp/sports_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['chunks', 'mp4'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'activity': 'Police officers walk together on a sidewalk',\n",
       "  'activity_interval': [2, 3],\n",
       "  'curr_interval': [0, 5],\n",
       "  'mp4_path': '/home/ubuntu/temp/mp4s/0.mp4'},\n",
       " {'activity': 'Police recruit #2 runs',\n",
       "  'activity_interval': [7, 8],\n",
       "  'curr_interval': [5, 10],\n",
       "  'mp4_path': '/home/ubuntu/temp/mp4s/0.mp4'},\n",
       " {'activity': 'Police recruits practice shooting',\n",
       "  'activity_interval': [9, 10],\n",
       "  'curr_interval': [10, 15],\n",
       "  'mp4_path': '/home/ubuntu/temp/mp4s/0.mp4'},\n",
       " {'activity': 'Police officers stand in formation',\n",
       "  'activity_interval': [6, 8],\n",
       "  'curr_interval': [15, 20],\n",
       "  'mp4_path': '/home/ubuntu/temp/mp4s/0.mp4'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(hf_dataset[0]['chunks']))\n",
    "hf_dataset[0]['chunks'][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_hf = DataLoader(hf_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "batch = next(iter(dataloader_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_pad(batch, tokenizer):\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "    max_len = max([len(b_item[\"input_ids\"][0]) for b_item in batch])\n",
    "    batch_dict = {\"input_ids\": [], \"attention_mask\": []}\n",
    "    # input_ids\n",
    "    for b_item in batch:\n",
    "        # Pad the input_ids to the max length\n",
    "        input_ids = b_item[\"input_ids\"]\n",
    "        padding_length = max_len - len(input_ids[0])\n",
    "        input_ids = torch.cat([input_ids[0], pad_token*torch.ones(padding_length, dtype=torch.long)])\n",
    "        batch_dict[\"input_ids\"].append(input_ids)\n",
    "    batch_dict[\"input_ids\"] = torch.stack(batch_dict[\"input_ids\"])\n",
    "    # attention_mask\n",
    "    for b_item in batch:\n",
    "        # Pad the attention_mask to the max length\n",
    "        attention_mask = b_item[\"attention_mask\"]\n",
    "        padding_length = max_len - len(attention_mask[0])\n",
    "        attention_mask = torch.cat([attention_mask[0], torch.zeros(padding_length, dtype=torch.long)])\n",
    "        batch_dict[\"attention_mask\"].append(attention_mask)\n",
    "    batch_dict[\"attention_mask\"] = torch.stack(batch_dict[\"attention_mask\"])\n",
    "\n",
    "    # assum no padding required for image related items\n",
    "    for key in [\"pixel_values\", \"grid_sizes\", \"merge_sizes\", \"modals\"]:\n",
    "        if key in batch[0]:\n",
    "            batch_dict[key] = batch[0][key]\n",
    "        #     if(key != \"modals\"):\n",
    "        #         batch_dict[key] = torch.stack([b_item[key] for b_item in batch])\n",
    "        #     else:\n",
    "        #         batch_dict[key] = [b_item[key] for b_item in batch]\n",
    "    return batch_dict\n",
    "\n",
    "\n",
    "def collate_fn(batch, tokenizer, num_memory_slots, max_chunks= 4, batchify=False, cuda=False):\n",
    "    # batchify the data\n",
    "    if batchify:\n",
    "        batch_dict = {}\n",
    "        batch_dict['mp4'] = [item['mp4'] for item in batch]\n",
    "        batch_dict['chunks'] = [item['chunks'] for item in batch]\n",
    "        batch = batch_dict\n",
    "    # Extract the relevant fields from the batch\n",
    "    chunks = batch['chunks']\n",
    "\n",
    "    full_batch_dict = {}\n",
    "    num_chunks = len(chunks) if len(chunks) < max_chunks else max_chunks\n",
    "    b_size = len(chunks[0]['activity'])\n",
    "    for t in range(num_chunks):\n",
    "        # Tokenize the instructions, time steps, questions, and answers\n",
    "        messages = [[{\"role\": \"user\", \"content\": \n",
    "                        [\n",
    "                            {\"type\": \"video\", \"video\": {\"video_path\": chunks[t]['mp4_path'][i], \"fps\": 1, \"max_frames\": 100, \n",
    "                                                        \"start_time\":chunks[t]['curr_interval'][0][0], \"end_time\":chunks[t]['curr_interval'][1][0]}},\n",
    "                            {\"type\": \"text\", \"text\": \" \"},\n",
    "                        ]},\n",
    "                    {\"role\": \"assistant\", \"content\": \"[MEMORY]\"*num_memory_slots}] for i in range(b_size)]\n",
    "        messages = [processor(conversation=message, return_tensors=\"pt\") for message in messages]\n",
    "        messages = collect_and_pad(messages, tokenizer)\n",
    "\n",
    "        qa = [[{\"role\": \"user\", \"content\": f\"What happened during during T: {chunks[t]['activity_interval'][0][0]}-{chunks[t]['activity_interval'][1][0]}s\"}, \n",
    "               {\"role\": \"assistant\", \"content\": chunks[t]['activity'][i]}] \n",
    "                    for i in range(b_size)]\n",
    "        qa = [processor(conversation=message, return_tensors=\"pt\") for message in qa]\n",
    "        qa = collect_and_pad(qa, tokenizer)\n",
    "\n",
    "        # # Move tokenized output to CUDA\n",
    "        # mem_tokenized = {key: torch.tensor(value) for key, value in mem_tokenized.items()}\n",
    "        # qa_tokenized = {key: torch.tensor(value) for key, value in qa_tokenized.items()}\n",
    "        # labels = labels\n",
    "        if(cuda):\n",
    "            messages = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in messages.items()}\n",
    "            if \"pixel_values\" in messages:\n",
    "                messages[\"pixel_values\"] = messages[\"pixel_values\"].to(torch.bfloat16)\n",
    "            qa = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in qa.items()}\n",
    "        \n",
    "        full_batch_dict[f\"T{t}\"] = {\n",
    "            'memory_save': messages,\n",
    "            'QA': qa,\n",
    "            # 'labels': labels,\n",
    "        }\n",
    "    return full_batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = collate_fn(batch, tokenizer, 2, cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,    872,    198,  ...,     60, 151645,    198]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'),\n",
       " 'pixel_values': tensor([[ 0.1299,  0.2559,  0.2637,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         [ 0.3105,  0.3105,  0.3418,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         [ 0.2949,  0.4277,  0.4355,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         ...,\n",
       "         [-1.0000, -1.0000, -1.0000,  ..., -0.5625, -0.5625, -0.5625],\n",
       "         [ 0.1924,  0.2002,  0.1924,  ..., -0.4512, -0.4824, -0.4746],\n",
       "         [-0.6250, -0.4434, -1.0000,  ..., -0.5312, -0.5781, -0.5781]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'grid_sizes': tensor([[ 5, 26, 46]], device='cuda:0'),\n",
       " 'merge_sizes': tensor([2], device='cuda:0'),\n",
       " 'modals': ['video']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input[\"T0\"]['memory_save']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1551]), torch.Size([1, 1551]), torch.Size([5980, 588]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input[\"T0\"]['memory_save']['input_ids'].shape, batch_input[\"T0\"]['memory_save']['attention_mask'].shape, batch_input[\"T0\"]['memory_save']['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.forward(**batch_input[\"T0\"][\"QA\"], return_dict=True, use_cache=True, past_key_values=a.past_key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 4.0000, -2.1406, -0.9297,  ..., -1.8125, -1.8125, -1.8125],\n",
       "         [ 6.5312,  4.3438,  3.6094,  ..., -1.5469, -1.5469, -1.5469],\n",
       "         [ 6.5938, 11.2500,  8.2500,  ..., -2.8750, -2.8750, -2.8750],\n",
       "         ...,\n",
       "         [13.2500,  6.9375,  3.1406,  ..., -0.3633, -0.3633, -0.3633],\n",
       "         [ 9.0000,  6.8750,  6.9375,  ..., -2.3594, -2.3594, -2.3594],\n",
       "         [ 6.6875,  7.7500,  8.1875,  ..., -1.2734, -1.2734, -1.2734]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>), past_key_values=DynamicCache(), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicCache()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.past_key_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input[\"T0\"]['QA']['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Videollama3Qwen2ForCausalLM(\n",
       "  (model): Videollama3Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "    (vision_encoder): Videollama3VisionEncoderModel(\n",
       "      (embeddings): Videollama3VisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "      )\n",
       "      (encoder): Videollama3VisionTransformerEncoder(\n",
       "        (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x Videollama3VisionEncoderLayer(\n",
       "            (self_attn): VisionFlashAttention2(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Videollama3VisionMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (mm_projector): MlpGeluProjector(\n",
       "      (readout): Sequential(\n",
       "        (0): Linear(in_features=1152, out_features=1536, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<|im_start|>',\n",
       "  'user',\n",
       "  'Ċ',\n",
       "  'What',\n",
       "  'Ġhappened',\n",
       "  'Ġduring',\n",
       "  'Ġduring',\n",
       "  'ĠT',\n",
       "  ':',\n",
       "  'Ġ',\n",
       "  '0',\n",
       "  '-',\n",
       "  '1',\n",
       "  's',\n",
       "  '<|im_end|>',\n",
       "  'Ċ',\n",
       "  '<|im_start|>',\n",
       "  'assistant',\n",
       "  'Ċ',\n",
       "  'Police',\n",
       "  'Ġrecruits',\n",
       "  'Ġpractice',\n",
       "  'Ġcombat',\n",
       "  '<|im_end|>',\n",
       "  'Ċ'],\n",
       " tensor([151644,    872,    198,   3838,   6932,   2337,   2337,    350,     25,\n",
       "            220,     15,     12,     16,     82, 151645,    198, 151644,  77091,\n",
       "            198,  22202,  55097,   6588,  12610, 151645,    198],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruct_tokens = tokenizer.convert_ids_to_tokens(batch_input[\"T0\"]['QA']['input_ids'][0])\n",
    "reconstruct_tokens, batch_input[\"T0\"]['QA']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['assistant'], 77091)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([77091]), tokenizer.convert_tokens_to_ids(\"assistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
