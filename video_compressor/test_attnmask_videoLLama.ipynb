{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 23:42:47.846871: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739835767.904320  228355 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739835767.933498  228355 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, AutoModel, AutoImageProcessor\n",
    "\n",
    "model_name = \"DAMO-NLP-SG/VideoLLaMA3-2B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "video_path = \"put your video path here\"\n",
    "image_path = \"put your image path here\"\n",
    "question = \"Describe this video in detail.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVideoLLaMA(AutoModelForCausalLM):\n",
    "    def forward(self, **kwargs):\n",
    "        \"\"\" Modify this function to implement custom logic in the forward pass. \"\"\"\n",
    "        print(\"Custom Forward Pass Executed\")\n",
    "        \n",
    "        # Extract input tensors\n",
    "        input_ids = kwargs.get(\"input_ids\", None)\n",
    "        pixel_values = kwargs.get(\"pixel_values\", None)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            print(f\"Input IDs Shape: {input_ids.shape}\")\n",
    "\n",
    "        if pixel_values is not None:\n",
    "            print(f\"Pixel Values Shape: {pixel_values.shape}\")\n",
    "\n",
    "        # Call the original forward pass (if needed)\n",
    "        outputs = super().forward(**kwargs)\n",
    "\n",
    "        # Modify the output logits (Example: applying a mask)\n",
    "        logits = outputs.logits\n",
    "        logits[:, :, -1] = -float(\"inf\")  # Example: Masking the last token\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Load the model\n",
    "model_name = \"DAMO-NLP-SG/VideoLLaMA3-2B\"\n",
    "model = CustomVideoLLaMA.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, **kwargs):\n",
    "    input_ids = kwargs.get(\"input_ids\", None)\n",
    "    attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "    \n",
    "    # Create KV-cache simulation mask\n",
    "    if attention_mask is not None:\n",
    "        # Keep only the first token's attention active\n",
    "        batch_size, seq_len = attention_mask.shape\n",
    "        kv_mask = torch.zeros_like(attention_mask)\n",
    "        kv_mask[:, 0] = 1  # Only first token attends\n",
    "        kwargs[\"attention_mask\"] = kv_mask\n",
    "    \n",
    "    # Call parent's forward pass\n",
    "    outputs = super().forward(**kwargs)\n",
    "    \n",
    "    # Restore original attention mask for future passes\n",
    "    if attention_mask is not None:\n",
    "        kwargs[\"attention_mask\"] = attention_mask\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video_path = \"/home/ubuntu/Tiananmen.mp4\"\n",
    "image_path = \"/home/ubuntu/tankman.png\"\n",
    "question = \"Which country is this from? Is this a monumental historic event? What is the historical name for this event\"\n",
    "\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful and informed assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"video\", \"video\": {\"video_path\": video_path, \"fps\": 1, \"max_frames\": 1000,}},\n",
    "            {\"type\": \"image\", \"image\": {\"image_path\" : image_path}},\n",
    "            {\"type\": \"text\", \"text\": question},\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reporter mentions that this is a video from 1989, and the footage shows tanks moving down a street in what appears to be a city. The commentator also confirms that it's China, as he points out the Chinese flag on one of the screens. Therefore, we can infer that this event took place in China during the year 1989, which was a significant period marked by political upheaval and protests.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = processor(conversation=conversation, return_tensors=\"pt\")\n",
    "inputs = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "if \"pixel_values\" in inputs:\n",
    "    inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(torch.bfloat16)\n",
    "\n",
    "# Forward pass with the modified model\n",
    "output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "response = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sequence length: 7585\n",
      "Maximum token length allowed in context window: 32768\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_228355/2637945090.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmax_token_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Maximum token length allowed in context window: {max_token_length}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Pixel Values Shape: {inputs['pixel_values'::].shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "# Get tokenized sequence length\n",
    "token_length = inputs[\"input_ids\"].shape[1]\n",
    "print(f\"Tokenized sequence length: {token_length}\")\n",
    "\n",
    "# Get the max token length from model config\n",
    "max_token_length = model.config.max_position_embeddings if hasattr(model.config, \"max_position_embeddings\") else None\n",
    "\n",
    "if max_token_length:\n",
    "    print(f\"Maximum token length allowed in context window: {max_token_length}\")\n",
    "    print(f\"Pixel Values Shape: {inputs['pixel_values'::].shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"Model does not have a specified max_position_embeddings attribute.\")\n",
    "# Check if pixel_values exist in inputs\n",
    "if \"pixel_values\" in inputs:\n",
    "    print(f\"Pixel Values Shape: {inputs['pixel_values'].shape}\")\n",
    "\n",
    "# Check model configuration for vision encoder limits\n",
    "if hasattr(model.config, \"vision_config\"):\n",
    "    vision_config = model.config.vision_config\n",
    "    max_image_token_length = vision_config.image_size if hasattr(vision_config, \"image_size\") else None\n",
    "    max_patch_tokens = vision_config.max_position_embeddings if hasattr(vision_config, \"max_position_embeddings\") else None\n",
    "\n",
    "    print(f\"Max Image Size (Height x Width): {max_image_token_length}\")\n",
    "    print(f\"Max Image Patch Tokens Allowed: {max_patch_tokens}\")\n",
    "else:\n",
    "    print(\"Model does not have a vision_config attribute. Check documentation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,  ...,     30, 151645,    198]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'),\n",
       " 'pixel_values': tensor([[-0.1455, -0.1455, -0.1377,  ..., -0.4668, -0.4746, -0.4824],\n",
       "         [-0.0747, -0.0747, -0.0747,  ..., -0.2793, -0.2637, -0.2793],\n",
       "         [-0.3730, -0.3809, -0.4043,  ..., -0.3027, -0.3105, -0.3105],\n",
       "         ...,\n",
       "         [-0.6719, -0.6562, -0.6484,  ..., -0.6562, -0.6484, -0.6484],\n",
       "         [-0.6172, -0.6016, -0.5938,  ..., -0.6250, -0.6328, -0.6406],\n",
       "         [-0.6719, -0.6719, -0.6719,  ..., -0.6562, -0.6562, -0.6562]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'grid_sizes': tensor([[20, 26, 46],\n",
       "         [ 1, 45, 29]], device='cuda:0'),\n",
       " 'merge_sizes': tensor([2, 1], device='cuda:0'),\n",
       " 'modals': ['video', 'image']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
